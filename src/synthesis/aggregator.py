"""
Pure Synthesis Aggregator

Replicates and enhances `perplexity_research` role in synthesis workflows.

Key difference from search+synthesis:
- Does NOT perform searches
- Takes PRE-GATHERED content from Ref/Exa/Jina
- Weaves into coherent narrative with attribution

This is the final step in the synthesis workflow:
Triple Stack (Ref + Exa + Jina) → This Aggregator → Final Output
"""

import re
from dataclasses import dataclass, field
from enum import Enum
from typing import Optional

from ..config import settings
from ..llm_utils import get_llm_content


class SynthesisStyle(str, Enum):
    """Style of synthesis output."""
    COMPREHENSIVE = "comprehensive"  # Full analysis with sections
    CONCISE = "concise"             # Brief, focused answer
    COMPARATIVE = "comparative"      # Side-by-side analysis
    TUTORIAL = "tutorial"           # Step-by-step guide format
    ACADEMIC = "academic"           # Scholarly tone with citations


@dataclass
class PreGatheredSource:
    """A source that was pre-fetched by Ref/Exa/Jina."""
    origin: str           # "ref", "exa", "jina", or custom
    url: str
    title: str
    content: str          # Full content (already fetched)
    source_type: str      # "documentation", "code", "article", etc.
    metadata: dict = field(default_factory=dict)


@dataclass
class AggregatedSynthesis:
    """Result of synthesis aggregation."""
    content: str
    citations: list[dict]
    source_attribution: dict[str, float]  # origin -> contribution %
    confidence: float
    style_used: SynthesisStyle
    word_count: int


# Synthesis prompts optimized for aggregation (not search)
COMPREHENSIVE_SYNTHESIS_PROMPT = """You are synthesizing research findings from multiple pre-gathered sources.

Query: {query}

Sources have been gathered from:
- Documentation (Ref): Official docs, API references
- Code Context (Exa): Code examples, implementations
- Web Content (Jina): Articles, discussions, tutorials

Pre-gathered content:
{sources}

Instructions:
1. Synthesize these sources into a comprehensive response
2. Use inline citations [1], [2], etc. corresponding to source numbers
3. Every factual claim MUST have a citation
4. Highlight where sources agree and where they differ
5. Note any gaps in the available information
6. Structure with clear sections if content warrants it

Provide a thorough synthesis:"""

CONCISE_SYNTHESIS_PROMPT = """Synthesize these pre-gathered sources into a focused, concise answer.

Query: {query}

Sources:
{sources}

Instructions:
1. Provide a direct, concise answer (2-4 paragraphs max)
2. Use inline citations [1], [2], etc.
3. Focus on the most important points
4. Skip tangential information

Concise synthesis:"""

COMPARATIVE_SYNTHESIS_PROMPT = """Create a comparative analysis from these pre-gathered sources.

Query: {query}

Sources:
{sources}

Instructions:
1. Identify the items/approaches being compared
2. Create a structured comparison (can use tables if helpful)
3. Cite sources for each comparison point [1], [2], etc.
4. Conclude with situational recommendations

Comparative analysis:"""

ACADEMIC_SYNTHESIS_PROMPT = """Synthesize these sources in an academic/scholarly style.

Query: {query}

Sources:
{sources}

Instructions:
1. Use formal, scholarly tone
2. Cite sources rigorously [1], [2], etc.
3. Acknowledge limitations and uncertainties
4. Structure as: Background → Analysis → Discussion → Conclusions

Academic synthesis:"""


class SynthesisAggregator:
    """
    Pure synthesis aggregator for pre-gathered content.

    Optimized for the specific role in synthesis workflows:
    - Input: Content already fetched by Ref/Exa/Jina
    - Output: Coherent synthesis with attribution
    - NO additional searching

    This mirrors `perplexity_research` functionality.
    """

    STYLE_PROMPTS = {
        SynthesisStyle.COMPREHENSIVE: COMPREHENSIVE_SYNTHESIS_PROMPT,
        SynthesisStyle.CONCISE: CONCISE_SYNTHESIS_PROMPT,
        SynthesisStyle.COMPARATIVE: COMPARATIVE_SYNTHESIS_PROMPT,
        SynthesisStyle.ACADEMIC: ACADEMIC_SYNTHESIS_PROMPT,
        SynthesisStyle.TUTORIAL: COMPREHENSIVE_SYNTHESIS_PROMPT,  # Use comprehensive as base
    }

    def __init__(
        self,
        llm_client,
        model: str = None,
    ):
        """
        Initialize the aggregator.

        Args:
            llm_client: OpenAI-compatible LLM client
            model: Model name for synthesis
        """
        self.llm_client = llm_client
        self.model = model or settings.llm_model

    async def synthesize(
        self,
        query: str,
        sources: list[PreGatheredSource],
        style: SynthesisStyle = SynthesisStyle.COMPREHENSIVE,
        max_tokens: int = 3000,
    ) -> AggregatedSynthesis:
        """
        Synthesize pre-gathered sources into coherent output.

        Args:
            query: The original research query
            sources: Pre-gathered sources from Ref/Exa/Jina
            style: Synthesis style to use
            max_tokens: Maximum tokens for response

        Returns:
            AggregatedSynthesis with content and metadata
        """
        if not sources:
            return AggregatedSynthesis(
                content="No sources provided for synthesis.",
                citations=[],
                source_attribution={},
                confidence=0.0,
                style_used=style,
                word_count=0,
            )

        # Format sources for prompt
        formatted_sources = self._format_sources(sources)

        # Select prompt based on style
        prompt_template = self.STYLE_PROMPTS.get(
            style, COMPREHENSIVE_SYNTHESIS_PROMPT
        )
        prompt = prompt_template.format(
            query=query,
            sources=formatted_sources,
        )

        # Generate synthesis
        response = await self._call_llm(prompt, max_tokens)

        # Extract citations and compute attribution
        citations = self._extract_citations(response, sources)
        attribution = self._compute_attribution(citations, sources)
        confidence = self._estimate_confidence(sources, citations)

        return AggregatedSynthesis(
            content=response,
            citations=citations,
            source_attribution=attribution,
            confidence=confidence,
            style_used=style,
            word_count=len(response.split()),
        )

    async def synthesize_with_reasoning(
        self,
        query: str,
        sources: list[PreGatheredSource],
        style: SynthesisStyle = SynthesisStyle.COMPREHENSIVE,
    ) -> AggregatedSynthesis:
        """
        Synthesize with explicit reasoning (mirrors perplexity_reason).

        Uses chain-of-thought to show reasoning process.
        """
        formatted_sources = self._format_sources(sources)

        prompt = f"""You are synthesizing research findings with explicit reasoning.

Query: {query}

Sources:
{formatted_sources}

First, think through your approach:
1. What are the key aspects of this query?
2. Which sources are most relevant to each aspect?
3. Are there any contradictions between sources?
4. What can be confidently stated vs what is uncertain?

<reasoning>
[Your step-by-step reasoning here]
</reasoning>

Now provide your synthesis based on this reasoning:

<synthesis>
[Your synthesized response with citations [1], [2], etc.]
</synthesis>"""

        response = await self._call_llm(prompt, max_tokens=4000)

        # Extract just the synthesis portion
        synthesis_match = re.search(
            r'<synthesis>(.*?)</synthesis>',
            response,
            re.DOTALL
        )
        content = synthesis_match.group(1).strip() if synthesis_match else response

        citations = self._extract_citations(content, sources)
        attribution = self._compute_attribution(citations, sources)

        return AggregatedSynthesis(
            content=content,
            citations=citations,
            source_attribution=attribution,
            confidence=self._estimate_confidence(sources, citations),
            style_used=style,
            word_count=len(content.split()),
        )

    def _format_sources(
        self,
        sources: list[PreGatheredSource],
        max_content_per_source: int = 2000,
    ) -> str:
        """Format sources for inclusion in prompt."""
        parts = []

        for i, source in enumerate(sources, 1):
            content = source.content[:max_content_per_source]
            if len(source.content) > max_content_per_source:
                content += "..."

            part = f"""[{i}] {source.title}
Origin: {source.origin} | Type: {source.source_type}
URL: {source.url}
Content:
{content}
---"""
            parts.append(part)

        return "\n\n".join(parts)

    def _extract_citations(
        self,
        text: str,
        sources: list[PreGatheredSource],
    ) -> list[dict]:
        """Extract citations from synthesized text."""
        citations = []
        seen = set()

        # Find all [N] patterns
        pattern = re.compile(r'\[(\d+)\]')
        for match in pattern.finditer(text):
            try:
                idx = int(match.group(1)) - 1  # Convert to 0-indexed
                if 0 <= idx < len(sources) and idx not in seen:
                    source = sources[idx]
                    citations.append({
                        "number": idx + 1,
                        "title": source.title,
                        "url": source.url,
                        "origin": source.origin,
                        "source_type": source.source_type,
                    })
                    seen.add(idx)
            except (ValueError, IndexError):
                continue

        return citations

    def _compute_attribution(
        self,
        citations: list[dict],
        sources: list[PreGatheredSource],
    ) -> dict[str, float]:
        """Compute attribution breakdown by source origin."""
        if not citations:
            return {}

        origin_counts = {}
        for citation in citations:
            origin = citation.get("origin", "unknown")
            origin_counts[origin] = origin_counts.get(origin, 0) + 1

        total = sum(origin_counts.values())
        return {
            origin: count / total
            for origin, count in origin_counts.items()
        }

    def _estimate_confidence(
        self,
        sources: list[PreGatheredSource],
        citations: list[dict],
    ) -> float:
        """Estimate confidence based on source quality and citation coverage."""
        if not sources:
            return 0.0

        # Base confidence from number of sources
        source_confidence = min(len(sources) / 5, 1.0) * 0.3

        # Citation coverage
        citation_ratio = len(citations) / max(len(sources), 1)
        citation_confidence = min(citation_ratio, 1.0) * 0.3

        # Source diversity (different origins)
        origins = set(s.origin for s in sources)
        diversity_confidence = min(len(origins) / 3, 1.0) * 0.2

        # Source type quality
        quality_types = {"documentation", "academic", "official"}
        quality_sources = [
            s for s in sources
            if s.source_type.lower() in quality_types
        ]
        quality_confidence = min(len(quality_sources) / max(len(sources), 1), 1.0) * 0.2

        return source_confidence + citation_confidence + diversity_confidence + quality_confidence

    async def _call_llm(self, prompt: str, max_tokens: int = 3000) -> str:
        """Call LLM with prompt."""
        response = await self.llm_client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=max_tokens,
            temperature=0.7,
        )
        return get_llm_content(response.choices[0].message)
